{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"522LSTM","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"_fZvH2LiPeSt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"outputId":"6ad26776-d51e-48a4-e23a-e300f49ae83a","executionInfo":{"status":"ok","timestamp":1576106185008,"user_tz":480,"elapsed":23983,"user":{"displayName":"DEQI WAN","photoUrl":"","userId":"06265948088351130092"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kRc1aIPOP-wt","colab_type":"code","colab":{}},"source":["import os\n","os.chdir(\"drive/My Drive/Colab Notebooks\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hIRccYXZiknL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":170},"outputId":"a76fe55b-60b1-4b89-9a63-ee3c367b9eaf","executionInfo":{"status":"ok","timestamp":1576106237030,"user_tz":480,"elapsed":1459,"user":{"displayName":"DEQI WAN","photoUrl":"","userId":"06265948088351130092"}}},"source":["ls"],"execution_count":11,"outputs":[{"output_type":"stream","text":[" 522LSTM.ipynb                             glove.840B.300d.gensim\n","'Copy2 of HW2.ipynb'                       glove.840B.300d.gensim.vectors.npy\n","'Copy3 of HW2.ipynb'                       nn_results.csv\n","'Copy of bert_text_classification.ipynb'   pro_df_nona.csv\n","'Copy of HW2.ipynb'                        \u001b[0m\u001b[01;34m__pycache__\u001b[0m/\n"," crawl-300d-2M.gensim                      test.csv\n"," crawl-300d-2M.gensim.vectors.npy          toxicity_debiasing_data.tsv\n"," generate_data.py                          train.csv\n"," Glove\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"17VzMg5u47kV","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","from itertools import product\n","df = pd.read_csv(\"pro_df_nona.csv\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YxlIihi_HUw_","colab_type":"code","colab":{}},"source":["df['load'] = df['load']/10000\n","for i in product(['load'],range(1,73,1), repeat=1):\n","    df[i[0]+'lag'+str(i[1])] = df[i[0]].shift(i[1])\n","df['trend'] = range(1,1+len(df))\n","df['trend'] = (df['trend']-np.min(df['trend']))/((np.max(df['trend'])-np.min(df['trend'])))\n","df = df.dropna()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BJ2j9zWwT1tQ","colab_type":"code","outputId":"cd19e8c3-b18a-4e79-d4a8-8fb5e39b52cc","executionInfo":{"status":"ok","timestamp":1576106247520,"user_tz":480,"elapsed":680,"user":{"displayName":"DEQI WAN","photoUrl":"","userId":"06265948088351130092"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["len(df)"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["49518"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"Ud-KFefY5zZ9","colab_type":"code","outputId":"0d6d4d53-b192-41eb-ef36-ef6a7303838a","executionInfo":{"status":"ok","timestamp":1576106247522,"user_tz":480,"elapsed":480,"user":{"displayName":"DEQI WAN","photoUrl":"","userId":"06265948088351130092"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# df = pd.read_csv(\"pro_df_nona.csv\")\n","train_df = df[df['date']<'2017-01-01']\n","test_df = df[df['date']>='2017-01-01']\n","print(train_df.shape)\n","print(test_df.shape)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["(26160, 388)\n","(23358, 388)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"teHucYIo6eRN","colab_type":"code","colab":{}},"source":["train_y = np.array(train_df['load18'])\n","test_y = np.array(test_df['load18'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z0hHNdws5_t3","colab_type":"code","colab":{}},"source":["del train_df['load18']\n","del test_df['load18']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"64WtJOM_6r_2","colab_type":"code","colab":{}},"source":["train_x = np.array(train_df[train_df.columns[4:]])\n","test_x = np.array(test_df[test_df.columns[4:]])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7BOW5sin-T0F","colab_type":"code","outputId":"83017bc1-c799-42d9-d65b-91f553f9a341","executionInfo":{"status":"ok","timestamp":1576106249003,"user_tz":480,"elapsed":1016,"user":{"displayName":"DEQI WAN","photoUrl":"","userId":"06265948088351130092"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["train_x.shape"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(26160, 383)"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"5exw9k2ULKx2","colab_type":"code","outputId":"b5a35deb-35aa-468f-d8f0-810f89c4f518","executionInfo":{"status":"ok","timestamp":1576106249005,"user_tz":480,"elapsed":837,"user":{"displayName":"DEQI WAN","photoUrl":"","userId":"06265948088351130092"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["test_x.shape"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(23358, 383)"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"f3pICVhj880k","colab_type":"code","colab":{}},"source":["import torch\n","X_train = torch.from_numpy(train_x).type(torch.Tensor).view(26160, 383)\n","X_test = torch.from_numpy(test_x).type(torch.Tensor).view(23358, 383)\n","y_train = torch.from_numpy(train_y).type(torch.Tensor).view(26160, 1)\n","y_test = torch.from_numpy(test_y).type(torch.Tensor).view(23358, 1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3LzCoCxmQZuY","colab_type":"code","outputId":"cc550c21-7f91-4c9a-fa3a-06181f96f457","executionInfo":{"status":"ok","timestamp":1576106279854,"user_tz":480,"elapsed":362,"user":{"displayName":"DEQI WAN","photoUrl":"","userId":"06265948088351130092"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["y_train"],"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[13378.],\n","        [13129.],\n","        [12743.],\n","        ...,\n","        [ 9322.],\n","        [ 9854.],\n","        [11245.]])"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"Zr1B_8hSP-4n","colab_type":"code","colab":{}},"source":["y_train=y_train/10000\n","y_test=y_test/10000"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1XNe-tyVLRpT","colab_type":"code","outputId":"02d2d92e-24f2-41dd-ce20-e28b6710ac67","executionInfo":{"status":"ok","timestamp":1576106281412,"user_tz":480,"elapsed":425,"user":{"displayName":"DEQI WAN","photoUrl":"","userId":"06265948088351130092"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["y_train"],"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1.3378],\n","        [1.3129],\n","        [1.2743],\n","        ...,\n","        [0.9322],\n","        [0.9854],\n","        [1.1245]])"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"4iWIX9NeHOeL","colab_type":"code","colab":{}},"source":["class Feedforward(torch.nn.Module):\n","        def __init__(self, input_size, hidden_size):\n","            super(Feedforward, self).__init__()\n","            self.input_size = input_size\n","            self.hidden_size  = hidden_size\n","            self.fc1 = torch.nn.Linear(self.input_size, 150)\n","            self.relu = torch.nn.ReLU()\n","            self.fc2 = torch.nn.Linear(150, 100)\n","            self.fc3 = torch.nn.Linear(100, 50)\n","            self.fc4 = torch.nn.Linear(50, 1)\n","            self.sigmoid = torch.nn.Sigmoid()\n","            self.dropout = torch.nn.Dropout(p=0.2)\n","        def forward(self, x):\n","            x = self.fc1(x)\n","            x = self.relu(x)\n","            x = self.dropout(x)\n","            x = self.fc2(x)\n","            x = self.relu(x)\n","            x = self.dropout(x)\n","            x = self.fc3(x)\n","            x = self.relu(x)\n","            x = self.dropout(x)\n","\n","            output = self.fc4(x)\n","            return output"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"V9JFnIpfHOs1","colab_type":"code","colab":{}},"source":["model = Feedforward(383, 200)\n","# criterion = torch.nn.BCELoss()\n","# optimizer = torch.optim.SGD(model.parameters(), lr = 0.0001)\n","\n","criterion = torch.nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"T3MKESz_HOvg","colab_type":"code","outputId":"db98aec1-7327-4021-8fb6-16df6d0fc2cc","executionInfo":{"status":"ok","timestamp":1576106285194,"user_tz":480,"elapsed":340,"user":{"displayName":"DEQI WAN","photoUrl":"","userId":"06265948088351130092"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["model.eval()\n","y_pred = model(X_test)\n","before_train = criterion(y_pred.squeeze(), y_test.squeeze())\n","print('Test loss before training' , before_train.item())"],"execution_count":35,"outputs":[{"output_type":"stream","text":["Test loss before training 1.1072627305984497\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5XRsumvKKDvH","colab_type":"code","outputId":"a22e2407-1b79-4af3-e911-4743ad8a95a2","executionInfo":{"status":"ok","timestamp":1576106287250,"user_tz":480,"elapsed":325,"user":{"displayName":"DEQI WAN","photoUrl":"","userId":"06265948088351130092"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["y_pred"],"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.1712],\n","        [0.1638],\n","        [0.1643],\n","        ...,\n","        [0.1301],\n","        [0.1312],\n","        [0.1424]], grad_fn=<AddmmBackward>)"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"code","metadata":{"id":"MBTApRpS885C","colab_type":"code","outputId":"f37b9a5c-575d-4a44-e4d7-bceaffa2720f","executionInfo":{"status":"ok","timestamp":1576107444352,"user_tz":480,"elapsed":1156708,"user":{"displayName":"DEQI WAN","photoUrl":"","userId":"06265948088351130092"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["model.train()\n","epoch = 300\n","for epoch in range(epoch):\n","    optimizer.zero_grad()\n","    # Forward pass\n","    y_pred = model(X_train)\n","    # Compute Loss\n","    loss = criterion(y_pred.squeeze(), y_train)\n","\n","    # model.eval()\n","    # y_pred_test = model(X_test)\n","    # after_train = criterion(y_pred_test.squeeze(), y_test) \n","    # print('Test loss after Training' , after_train.item())  \n","   \n","    print('Epoch {}: train loss: {}'.format(epoch, loss.item()))\n","    # Backward pass\n","    loss.backward()\n","    optimizer.step()"],"execution_count":37,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([26160, 1])) that is different to the input size (torch.Size([26160])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 0: train loss: 1.1369261741638184\n","Epoch 1: train loss: 1.1186665296554565\n","Epoch 2: train loss: 1.099485993385315\n","Epoch 3: train loss: 1.081811547279358\n","Epoch 4: train loss: 1.0638928413391113\n","Epoch 5: train loss: 1.0443737506866455\n","Epoch 6: train loss: 1.0251432657241821\n","Epoch 7: train loss: 1.007296085357666\n","Epoch 8: train loss: 0.9871962070465088\n","Epoch 9: train loss: 0.9670699238777161\n","Epoch 10: train loss: 0.9470869898796082\n","Epoch 11: train loss: 0.926572859287262\n","Epoch 12: train loss: 0.9065247178077698\n","Epoch 13: train loss: 0.8849129676818848\n","Epoch 14: train loss: 0.863222062587738\n","Epoch 15: train loss: 0.8399873971939087\n","Epoch 16: train loss: 0.8173266649246216\n","Epoch 17: train loss: 0.7946793437004089\n","Epoch 18: train loss: 0.769955039024353\n","Epoch 19: train loss: 0.7482254505157471\n","Epoch 20: train loss: 0.7216757535934448\n","Epoch 21: train loss: 0.6974027752876282\n","Epoch 22: train loss: 0.6734885573387146\n","Epoch 23: train loss: 0.6466020941734314\n","Epoch 24: train loss: 0.6220256090164185\n","Epoch 25: train loss: 0.5948504209518433\n","Epoch 26: train loss: 0.568229615688324\n","Epoch 27: train loss: 0.5418521165847778\n","Epoch 28: train loss: 0.5144785046577454\n","Epoch 29: train loss: 0.48906388878822327\n","Epoch 30: train loss: 0.46433597803115845\n","Epoch 31: train loss: 0.43697458505630493\n","Epoch 32: train loss: 0.4093024730682373\n","Epoch 33: train loss: 0.3848007023334503\n","Epoch 34: train loss: 0.3584529459476471\n","Epoch 35: train loss: 0.3340749442577362\n","Epoch 36: train loss: 0.31024327874183655\n","Epoch 37: train loss: 0.28853508830070496\n","Epoch 38: train loss: 0.2677099406719208\n","Epoch 39: train loss: 0.24525044858455658\n","Epoch 40: train loss: 0.22733500599861145\n","Epoch 41: train loss: 0.21135693788528442\n","Epoch 42: train loss: 0.19504393637180328\n","Epoch 43: train loss: 0.181441530585289\n","Epoch 44: train loss: 0.16845491528511047\n","Epoch 45: train loss: 0.1581859141588211\n","Epoch 46: train loss: 0.14982308447360992\n","Epoch 47: train loss: 0.14265909790992737\n","Epoch 48: train loss: 0.13722048699855804\n","Epoch 49: train loss: 0.13456042110919952\n","Epoch 50: train loss: 0.13147233426570892\n","Epoch 51: train loss: 0.1314256191253662\n","Epoch 52: train loss: 0.13126099109649658\n","Epoch 53: train loss: 0.13184669613838196\n","Epoch 54: train loss: 0.1337423473596573\n","Epoch 55: train loss: 0.13505783677101135\n","Epoch 56: train loss: 0.1364179253578186\n","Epoch 57: train loss: 0.13705170154571533\n","Epoch 58: train loss: 0.13698723912239075\n","Epoch 59: train loss: 0.13756050169467926\n","Epoch 60: train loss: 0.1376178115606308\n","Epoch 61: train loss: 0.13730792701244354\n","Epoch 62: train loss: 0.13570678234100342\n","Epoch 63: train loss: 0.13351424038410187\n","Epoch 64: train loss: 0.13184545934200287\n","Epoch 65: train loss: 0.13070259988307953\n","Epoch 66: train loss: 0.1280122846364975\n","Epoch 67: train loss: 0.12613457441329956\n","Epoch 68: train loss: 0.12517467141151428\n","Epoch 69: train loss: 0.12336800992488861\n","Epoch 70: train loss: 0.12181264907121658\n","Epoch 71: train loss: 0.12213262170553207\n","Epoch 72: train loss: 0.12032634019851685\n","Epoch 73: train loss: 0.12046503275632858\n","Epoch 74: train loss: 0.11944036930799484\n","Epoch 75: train loss: 0.11898769438266754\n","Epoch 76: train loss: 0.11849962919950485\n","Epoch 77: train loss: 0.11798590421676636\n","Epoch 78: train loss: 0.11885373294353485\n","Epoch 79: train loss: 0.11836616694927216\n","Epoch 80: train loss: 0.11827858537435532\n","Epoch 81: train loss: 0.11836099624633789\n","Epoch 82: train loss: 0.11762332171201706\n","Epoch 83: train loss: 0.11641094088554382\n","Epoch 84: train loss: 0.11726753413677216\n","Epoch 85: train loss: 0.11593658477067947\n","Epoch 86: train loss: 0.11535248160362244\n","Epoch 87: train loss: 0.1151103600859642\n","Epoch 88: train loss: 0.11629196256399155\n","Epoch 89: train loss: 0.11493539810180664\n","Epoch 90: train loss: 0.1146123930811882\n","Epoch 91: train loss: 0.11360184103250504\n","Epoch 92: train loss: 0.11460477858781815\n","Epoch 93: train loss: 0.11341992020606995\n","Epoch 94: train loss: 0.11335522681474686\n","Epoch 95: train loss: 0.11275839060544968\n","Epoch 96: train loss: 0.11371641606092453\n","Epoch 97: train loss: 0.1126825287938118\n","Epoch 98: train loss: 0.11260650306940079\n","Epoch 99: train loss: 0.11208855360746384\n","Epoch 100: train loss: 0.11203364282846451\n","Epoch 101: train loss: 0.11212775111198425\n","Epoch 102: train loss: 0.11157309263944626\n","Epoch 103: train loss: 0.11136116087436676\n","Epoch 104: train loss: 0.11037939041852951\n","Epoch 105: train loss: 0.11134528368711472\n","Epoch 106: train loss: 0.1106131449341774\n","Epoch 107: train loss: 0.11100226640701294\n","Epoch 108: train loss: 0.11040914803743362\n","Epoch 109: train loss: 0.10955183953046799\n","Epoch 110: train loss: 0.10952426493167877\n","Epoch 111: train loss: 0.10989575833082199\n","Epoch 112: train loss: 0.10920925438404083\n","Epoch 113: train loss: 0.1095292791724205\n","Epoch 114: train loss: 0.10960177332162857\n","Epoch 115: train loss: 0.1089068129658699\n","Epoch 116: train loss: 0.10899421572685242\n","Epoch 117: train loss: 0.10855908691883087\n","Epoch 118: train loss: 0.1083264946937561\n","Epoch 119: train loss: 0.10805415362119675\n","Epoch 120: train loss: 0.10837792605161667\n","Epoch 121: train loss: 0.10801244527101517\n","Epoch 122: train loss: 0.10799147188663483\n","Epoch 123: train loss: 0.10736342519521713\n","Epoch 124: train loss: 0.1074288859963417\n","Epoch 125: train loss: 0.1075836643576622\n","Epoch 126: train loss: 0.10739769786596298\n","Epoch 127: train loss: 0.1065453588962555\n","Epoch 128: train loss: 0.10755221545696259\n","Epoch 129: train loss: 0.1063963770866394\n","Epoch 130: train loss: 0.1065017506480217\n","Epoch 131: train loss: 0.10677836090326309\n","Epoch 132: train loss: 0.1068238765001297\n","Epoch 133: train loss: 0.10689225047826767\n","Epoch 134: train loss: 0.10676218569278717\n","Epoch 135: train loss: 0.10615091025829315\n","Epoch 136: train loss: 0.10592605173587799\n","Epoch 137: train loss: 0.10612959414720535\n","Epoch 138: train loss: 0.10542605072259903\n","Epoch 139: train loss: 0.10576988756656647\n","Epoch 140: train loss: 0.10559383779764175\n","Epoch 141: train loss: 0.10562028735876083\n","Epoch 142: train loss: 0.10552927106618881\n","Epoch 143: train loss: 0.1051662489771843\n","Epoch 144: train loss: 0.10485700517892838\n","Epoch 145: train loss: 0.10506963729858398\n","Epoch 146: train loss: 0.10476478934288025\n","Epoch 147: train loss: 0.10488743335008621\n","Epoch 148: train loss: 0.10469954460859299\n","Epoch 149: train loss: 0.10425052791833878\n","Epoch 150: train loss: 0.10409747809171677\n","Epoch 151: train loss: 0.10484611988067627\n","Epoch 152: train loss: 0.10425612330436707\n","Epoch 153: train loss: 0.10427364706993103\n","Epoch 154: train loss: 0.10388056188821793\n","Epoch 155: train loss: 0.10382969677448273\n","Epoch 156: train loss: 0.10438507050275803\n","Epoch 157: train loss: 0.1039600744843483\n","Epoch 158: train loss: 0.10338469594717026\n","Epoch 159: train loss: 0.10336775332689285\n","Epoch 160: train loss: 0.103553906083107\n","Epoch 161: train loss: 0.10319216549396515\n","Epoch 162: train loss: 0.10319813340902328\n","Epoch 163: train loss: 0.10281115770339966\n","Epoch 164: train loss: 0.10279221832752228\n","Epoch 165: train loss: 0.10326436161994934\n","Epoch 166: train loss: 0.10293754935264587\n","Epoch 167: train loss: 0.10238850861787796\n","Epoch 168: train loss: 0.10281691700220108\n","Epoch 169: train loss: 0.10222965478897095\n","Epoch 170: train loss: 0.10164667665958405\n","Epoch 171: train loss: 0.10179036855697632\n","Epoch 172: train loss: 0.10176599025726318\n","Epoch 173: train loss: 0.10233920812606812\n","Epoch 174: train loss: 0.10183777660131454\n","Epoch 175: train loss: 0.10202611237764359\n","Epoch 176: train loss: 0.10174044966697693\n","Epoch 177: train loss: 0.1011568233370781\n","Epoch 178: train loss: 0.10209442675113678\n","Epoch 179: train loss: 0.10166039317846298\n","Epoch 180: train loss: 0.10165062546730042\n","Epoch 181: train loss: 0.10184524953365326\n","Epoch 182: train loss: 0.10135488212108612\n","Epoch 183: train loss: 0.10164076089859009\n","Epoch 184: train loss: 0.10131266713142395\n","Epoch 185: train loss: 0.10122569650411606\n","Epoch 186: train loss: 0.10115619003772736\n","Epoch 187: train loss: 0.10082422196865082\n","Epoch 188: train loss: 0.10087863355875015\n","Epoch 189: train loss: 0.10052963346242905\n","Epoch 190: train loss: 0.10013823211193085\n","Epoch 191: train loss: 0.10077928006649017\n","Epoch 192: train loss: 0.10030346363782883\n","Epoch 193: train loss: 0.10068739205598831\n","Epoch 194: train loss: 0.09994664043188095\n","Epoch 195: train loss: 0.1003635823726654\n","Epoch 196: train loss: 0.09997452795505524\n","Epoch 197: train loss: 0.1002061665058136\n","Epoch 198: train loss: 0.09968746453523636\n","Epoch 199: train loss: 0.09973938018083572\n","Epoch 200: train loss: 0.09984943270683289\n","Epoch 201: train loss: 0.09952528029680252\n","Epoch 202: train loss: 0.09956669807434082\n","Epoch 203: train loss: 0.09910858422517776\n","Epoch 204: train loss: 0.0996331050992012\n","Epoch 205: train loss: 0.0996507778763771\n","Epoch 206: train loss: 0.09931144118309021\n","Epoch 207: train loss: 0.0991576686501503\n","Epoch 208: train loss: 0.09952149540185928\n","Epoch 209: train loss: 0.09890180081129074\n","Epoch 210: train loss: 0.09895122796297073\n","Epoch 211: train loss: 0.09867528080940247\n","Epoch 212: train loss: 0.0987539067864418\n","Epoch 213: train loss: 0.09891051054000854\n","Epoch 214: train loss: 0.09849611669778824\n","Epoch 215: train loss: 0.09845931082963943\n","Epoch 216: train loss: 0.09859887510538101\n","Epoch 217: train loss: 0.09877622872591019\n","Epoch 218: train loss: 0.09812105447053909\n","Epoch 219: train loss: 0.09818579256534576\n","Epoch 220: train loss: 0.09811118245124817\n","Epoch 221: train loss: 0.09820053726434708\n","Epoch 222: train loss: 0.09832650423049927\n","Epoch 223: train loss: 0.09803241491317749\n","Epoch 224: train loss: 0.09808202087879181\n","Epoch 225: train loss: 0.0975695326924324\n","Epoch 226: train loss: 0.09791222214698792\n","Epoch 227: train loss: 0.09733457863330841\n","Epoch 228: train loss: 0.09794820100069046\n","Epoch 229: train loss: 0.09735321253538132\n","Epoch 230: train loss: 0.09717442095279694\n","Epoch 231: train loss: 0.09723106771707535\n","Epoch 232: train loss: 0.09718645364046097\n","Epoch 233: train loss: 0.09743326157331467\n","Epoch 234: train loss: 0.09733885526657104\n","Epoch 235: train loss: 0.09750601649284363\n","Epoch 236: train loss: 0.09689218550920486\n","Epoch 237: train loss: 0.09727167338132858\n","Epoch 238: train loss: 0.09717588871717453\n","Epoch 239: train loss: 0.09664086997509003\n","Epoch 240: train loss: 0.09653863310813904\n","Epoch 241: train loss: 0.09676988422870636\n","Epoch 242: train loss: 0.09688391536474228\n","Epoch 243: train loss: 0.09637072682380676\n","Epoch 244: train loss: 0.09646578133106232\n","Epoch 245: train loss: 0.09605127573013306\n","Epoch 246: train loss: 0.09666118770837784\n","Epoch 247: train loss: 0.09617254883050919\n","Epoch 248: train loss: 0.09603673219680786\n","Epoch 249: train loss: 0.09650218486785889\n","Epoch 250: train loss: 0.09595014154911041\n","Epoch 251: train loss: 0.09618309885263443\n","Epoch 252: train loss: 0.09600415080785751\n","Epoch 253: train loss: 0.09607260674238205\n","Epoch 254: train loss: 0.09570007026195526\n","Epoch 255: train loss: 0.0961708277463913\n","Epoch 256: train loss: 0.09546392410993576\n","Epoch 257: train loss: 0.09555713087320328\n","Epoch 258: train loss: 0.09615982323884964\n","Epoch 259: train loss: 0.09546519070863724\n","Epoch 260: train loss: 0.09504768997430801\n","Epoch 261: train loss: 0.09521447867155075\n","Epoch 262: train loss: 0.09530890733003616\n","Epoch 263: train loss: 0.09561201930046082\n","Epoch 264: train loss: 0.09479478746652603\n","Epoch 265: train loss: 0.09509796649217606\n","Epoch 266: train loss: 0.09526284039020538\n","Epoch 267: train loss: 0.09500711411237717\n","Epoch 268: train loss: 0.09501383453607559\n","Epoch 269: train loss: 0.09438672661781311\n","Epoch 270: train loss: 0.0945126935839653\n","Epoch 271: train loss: 0.09485924243927002\n","Epoch 272: train loss: 0.09437361359596252\n","Epoch 273: train loss: 0.09483904391527176\n","Epoch 274: train loss: 0.09464608877897263\n","Epoch 275: train loss: 0.0940723642706871\n","Epoch 276: train loss: 0.09480873495340347\n","Epoch 277: train loss: 0.09432323276996613\n","Epoch 278: train loss: 0.09424223750829697\n","Epoch 279: train loss: 0.09445769339799881\n","Epoch 280: train loss: 0.09438460320234299\n","Epoch 281: train loss: 0.09406400471925735\n","Epoch 282: train loss: 0.09384499490261078\n","Epoch 283: train loss: 0.09392731636762619\n","Epoch 284: train loss: 0.09389995783567429\n","Epoch 285: train loss: 0.09372778981924057\n","Epoch 286: train loss: 0.09392276406288147\n","Epoch 287: train loss: 0.09432341903448105\n","Epoch 288: train loss: 0.09377313405275345\n","Epoch 289: train loss: 0.09313563257455826\n","Epoch 290: train loss: 0.09326019883155823\n","Epoch 291: train loss: 0.09378007799386978\n","Epoch 292: train loss: 0.09367674589157104\n","Epoch 293: train loss: 0.09370443969964981\n","Epoch 294: train loss: 0.09333637356758118\n","Epoch 295: train loss: 0.0936109870672226\n","Epoch 296: train loss: 0.09337703138589859\n","Epoch 297: train loss: 0.09315336495637894\n","Epoch 298: train loss: 0.09311823546886444\n","Epoch 299: train loss: 0.09336017072200775\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zpt_rHZh883S","colab_type":"code","outputId":"5172c3cc-cd89-4d17-cbda-8ae02eadf5d6","executionInfo":{"status":"ok","timestamp":1576107445340,"user_tz":480,"elapsed":1153813,"user":{"displayName":"DEQI WAN","photoUrl":"","userId":"06265948088351130092"}},"colab":{"base_uri":"https://localhost:8080/","height":88}},"source":["model.eval()\n","y_pred_test = model(X_test)\n","after_train = criterion(y_pred_test.squeeze(), y_test) \n","print('Test loss after Training' , after_train.item())"],"execution_count":38,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([23358, 1])) that is different to the input size (torch.Size([23358])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"],"name":"stderr"},{"output_type":"stream","text":["Test loss after Training 0.07558967918157578\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZeJfZlRJTIHP","colab_type":"code","outputId":"3a6fbe00-59b9-4477-a428-2d444c67e9a8","executionInfo":{"status":"ok","timestamp":1576107445341,"user_tz":480,"elapsed":1152776,"user":{"displayName":"DEQI WAN","photoUrl":"","userId":"06265948088351130092"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["y_pred_test"],"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1.1242],\n","        [1.1282],\n","        [1.1378],\n","        ...,\n","        [1.1250],\n","        [1.1267],\n","        [1.1342]], grad_fn=<AddmmBackward>)"]},"metadata":{"tags":[]},"execution_count":39}]},{"cell_type":"code","metadata":{"id":"slNhta8HTIEw","colab_type":"code","outputId":"56ca69cb-af2e-4702-ee5f-ba5cdf00107a","executionInfo":{"status":"ok","timestamp":1576107445342,"user_tz":480,"elapsed":1152524,"user":{"displayName":"DEQI WAN","photoUrl":"","userId":"06265948088351130092"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["y_test*10000"],"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[11579.0000],\n","        [11579.0000],\n","        [11433.0010],\n","        ...,\n","        [15667.0000],\n","        [14515.0010],\n","        [13353.0000]])"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"code","metadata":{"id":"SsszzsgQTIB-","colab_type":"code","colab":{}},"source":["import numpy as np\n","\n","def mean_absolute_percentage_error(y_true, y_pred): \n","    y_true, y_pred = np.array(y_true), np.array(y_pred)\n","    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"U9YjBy_pZ0Op","colab_type":"code","outputId":"ae2a3e73-47b0-47b8-90ec-86fffd1515fe","executionInfo":{"status":"ok","timestamp":1576107445344,"user_tz":480,"elapsed":1139606,"user":{"displayName":"DEQI WAN","photoUrl":"","userId":"06265948088351130092"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["y_pred_test.detach().numpy()*10000"],"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[11241.623],\n","       [11281.637],\n","       [11378.151],\n","       ...,\n","       [11250.325],\n","       [11267.491],\n","       [11342.369]], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":42}]},{"cell_type":"code","metadata":{"id":"VQO55wFpTH_B","colab_type":"code","outputId":"feb8420f-8938-408f-8ab5-b81cb0980924","executionInfo":{"status":"ok","timestamp":1576107445345,"user_tz":480,"elapsed":1137836,"user":{"displayName":"DEQI WAN","photoUrl":"","userId":"06265948088351130092"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["mean_absolute_percentage_error(y_test.detach().numpy()*10000, y_pred_test.detach().numpy()*10000)"],"execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/plain":["15.589116513729095"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"code","metadata":{"id":"1ZI4dlmdTHz4","colab_type":"code","colab":{}},"source":["a=y_pred_test.detach().numpy()*10000"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VVuDbzWTCooe","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":119},"outputId":"3102b647-28ce-4ea8-f267-dd065782585c","executionInfo":{"status":"ok","timestamp":1576107699238,"user_tz":480,"elapsed":357,"user":{"displayName":"DEQI WAN","photoUrl":"","userId":"06265948088351130092"}}},"source":["test_df['pred_load18'] = a"],"execution_count":45,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  \"\"\"Entry point for launching an IPython kernel.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"MKpeoSPlCeRz","colab_type":"code","colab":{}},"source":["test_df.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vkIbhdIECxIO","colab_type":"code","colab":{}},"source":["test_df.to_csv(\"nn_results.csv\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2BPxjfztDCX0","colab_type":"code","colab":{}},"source":["final = test_df[['time','pred_load18']]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JQrQhuKNJQkp","colab_type":"code","colab":{}},"source":["final['deqi_NN_y'] = test_df['pred_load18'].shift(18)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"i500AOReKQ0u","colab_type":"code","colab":{}},"source":["# final[['time','deqi_NN_y']].to_csv(\"nn_results.csv\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jzcGHdAGKRoZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":119},"outputId":"12780fc4-1d68-4d5e-d5e3-f5de9d3f76eb","executionInfo":{"status":"ok","timestamp":1576107736878,"user_tz":480,"elapsed":375,"user":{"displayName":"DEQI WAN","photoUrl":"","userId":"06265948088351130092"}}},"source":["final['time'] = final['time'].shift(-18)"],"execution_count":48,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  \"\"\"Entry point for launching an IPython kernel.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"tNqiUJpsV4vY","colab_type":"code","colab":{}},"source":["final.to_csv(\"nn_results.csv\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MSslF-F_MndJ","colab_type":"code","colab":{}},"source":["pd.date_range('1 / 1/2018', periods = 3, freq ='MS')"],"execution_count":0,"outputs":[]}]}